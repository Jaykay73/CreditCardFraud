{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a1555a9-c70f-439b-943d-c6c989e3707a",
   "metadata": {},
   "source": [
    "# 02 – Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38bbf77-0fa2-47fe-b16e-23f3cabd3432",
   "metadata": {},
   "source": [
    "Scale numerical features, optionally apply PCA, and prepare train/test splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3022d4d-fb3d-4249-a649-8b20bed2dbf8",
   "metadata": {},
   "source": [
    "## 2.1 – Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e758eac-b157-4525-9c20-135810ce97a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import joblib\n",
    "\n",
    "# For reproducibility\n",
    "RANDOM_SEED = 31\n",
    "DATA_PATH = 'creditcard.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40a3e9b-be10-4a42-87c2-5528136b1c9f",
   "metadata": {},
   "source": [
    "## 2.2 – Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adcad76e-24a8-4dc4-bd7b-c6ca52364406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset shape: (284807, 31)\n",
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read CSV and inspect basic info\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"Full dataset shape: {df.shape}\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d20065c-dd2f-474c-88b1-aacd50968d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution:\n",
      " Class\n",
      "0    284315\n",
      "1       492\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check class balance\n",
    "print(\"\\nClass distribution:\\n\", df['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0c5aa2-64a8-40a5-b8f5-367e5a31e4cb",
   "metadata": {},
   "source": [
    "## 2.3 – Train/Test Split (Before Scaling)\n",
    "\n",
    "We stratify on the “Class” column to preserve the imbalance ratio in both splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c57367f8-495c-4e32-a211-9ff4ac03bbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df = df[df['Class'] == 1]\n",
    "normal_df = df[df['Class'] == 0].sample(n=3000, random_state=RANDOM_SEED)\n",
    "\n",
    "sample_df = pd.concat([fraud_df, normal_df], axis=0)\n",
    "X = sample_df.drop(columns=['Class'])\n",
    "y = sample_df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85f2a185-d42d-45b5-93fa-d4e884ac6bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.20,\n",
    "    stratify=y,\n",
    "    random_state=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1672dde-c9e0-49ed-99c4-71998f98251e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (2793, 30),  y_train: (2793,)\n",
      "X_test:  (699, 30),   y_test:  (699,)\n",
      "\n",
      "Train class distribution:\n",
      " Class\n",
      "0    2399\n",
      "1     394\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test class distribution:\n",
      " Class\n",
      "0    601\n",
      "1     98\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train: {X_train.shape},  y_train: {y_train.shape}\")\n",
    "print(f\"X_test:  {X_test.shape},   y_test:  {y_test.shape}\")\n",
    "print(\"\\nTrain class distribution:\\n\", y_train.value_counts())\n",
    "print(\"\\nTest class distribution:\\n\", y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f746851-c9b1-4d86-b857-53c87d252999",
   "metadata": {},
   "source": [
    "## 2.4 – Feature Scaling\n",
    "\n",
    "All “V1…V28” features are already PCA components provided by Kaggle.  \n",
    "We need to standardize “Amount” and “Time” **using only the training set** to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14923e0a-a6ce-4cdf-ad8d-138c729d9242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled feature stats (train set):\n",
      "      Amount_scaled   Time_scaled\n",
      "mean  -3.688818e-17 -6.741634e-17\n",
      "std    1.000179e+00  1.000179e+00\n"
     ]
    }
   ],
   "source": [
    "# Initialize scaler and fit on training “Amount” and “Time” only\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on train\n",
    "scaler.fit(X_train[['Amount', 'Time']])\n",
    "\n",
    "# Transform both train and test\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled  = X_test.copy()\n",
    "\n",
    "X_train_scaled[['Amount_scaled', 'Time_scaled']] = scaler.transform(X_train[['Amount', 'Time']])\n",
    "X_test_scaled[['Amount_scaled', 'Time_scaled']]  = scaler.transform(X_test[['Amount', 'Time']])\n",
    "\n",
    "# Now drop the original “Amount” and “Time” columns\n",
    "X_train_scaled = X_train_scaled.drop(columns=['Amount', 'Time'])\n",
    "X_test_scaled  = X_test_scaled.drop(columns=['Amount', 'Time'])\n",
    "\n",
    "# Sanity check: train “Amount_scaled” mean ~0, std ~1\n",
    "print(\"Scaled feature stats (train set):\")\n",
    "print(X_train_scaled[['Amount_scaled', 'Time_scaled']].describe().loc[['mean','std']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27643f08-aab3-4075-9843-9661a14e49f6",
   "metadata": {},
   "source": [
    "> **Notes:**\n",
    ">\n",
    "> * We created two new columns—`Amount_scaled` and `Time_scaled`—rather than overwriting, to keep clear which features have been processed.\n",
    "> * Dropping the original “Amount” and “Time” ensures our modeling pipelines only see scaled values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e24fbcd-7bf0-4a22-b939-abc09f3d715f",
   "metadata": {},
   "source": [
    "## 2.5 – PCA for Dimensionality Reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5c54d7-c919-41bb-a0e8-7a1222864238",
   "metadata": {},
   "source": [
    "In order to reduce dimensionality further (e.g., to speed up certain algorithms), we will run PCA on the training set (excluding “Class”) and then transform the test set. Below, we keep enough components to explain 95% of variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0edfedcd-0fc2-431f-a399-7648fec91f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare arrays for PCA: convert DataFrames to NumPy arrays\n",
    "X_train_for_pca = X_train_scaled.values\n",
    "X_test_for_pca  = X_test_scaled.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e87e4f1a-afff-407e-9c85-760338423213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA on training data\n",
    "pca = PCA(n_components=0.95, random_state=RANDOM_SEED)\n",
    "X_train_pca = pca.fit_transform(X_train_for_pca)\n",
    "X_test_pca  = pca.transform(X_test_for_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a121d74d-ea73-4985-80bf-81d9757b037d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature dims: 30\n",
      "PCA-reduced dims:     16  (95% variance retained)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original feature dims: {X_train_for_pca.shape[1]}\")\n",
    "print(f\"PCA-reduced dims:     {X_train_pca.shape[1]}  (95% variance retained)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df8255ff-caf0-48bb-9cd2-3735f0bde4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " - processed_scaled.pkl  (scaled, no PCA)\n",
      " - processed_pca.pkl     (scaled + PCA)\n"
     ]
    }
   ],
   "source": [
    "# Example: save both scaled (no PCA) and PCA-transformed versions\n",
    "joblib.dump((X_train_scaled, X_test_scaled, y_train, y_test), 'sample/processed_scaled.pkl')\n",
    "joblib.dump((X_train_pca, X_test_pca, y_train, y_test), 'sample/processed_pca.pkl')\n",
    "\n",
    "print(\"Saved:\\n - processed_scaled.pkl  (scaled, no PCA)\\n - processed_pca.pkl     (scaled + PCA)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1c5710-48ce-468d-ae40-84ca9ce53aec",
   "metadata": {},
   "source": [
    "### End of Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025a858d-1f28-4fd8-935e-15a375fcd042",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Summary of Changes & Improvements**\n",
    "\n",
    "1. **Train/Test Split Before Scaling**\n",
    "\n",
    "   * Ensures no data leakage by fitting the `StandardScaler` only on the training set.\n",
    "2. **Combined Scaling**\n",
    "\n",
    "   * Scaled “Amount” and “Time” together via a single `StandardScaler` call (instead of fitting separate scalers).\n",
    "3. **Clear Feature Columns**\n",
    "\n",
    "   * Created new columns `Amount_scaled` and `Time_scaled` and dropped the originals.\n",
    "4. **Optional PCA on Train/Test**\n",
    "\n",
    "   * Demonstrated how to fit PCA on training data and transform both train/test.\n",
    "5. **Saving to Disk**\n",
    "\n",
    "   * Showed how to pickle both the scaled/no-PCA and scaled+PCA versions for faster iteration.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
